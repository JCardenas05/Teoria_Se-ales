# -*- coding: utf-8 -*-
"""Bynary_detec (1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vzlPCO_nX8kXFkM_BeYZdXAI2iFj7lEc

# Taller 4
- Natalia Rosario Vasquez

### Tarea:

- Lograr crear un método en el cual la máquina pueda identificar cuándo se trata de un 0 o un 1. En este proceso, se toma la señal, se le introduce ruido y se le comunica a la máquina que esto representa un cero. Luego, se añade el ruido, y la máquina debe ajustar los parámetros de manera que se minimice la probabilidad de detectar incorrectamente estos símbolos.

### Detección de señal binaria

Conjuntos que representan un número finito de objetos/estados o relaciones indexados (símbolos)


$
		{\textrm{Tx: } \{p(a_n)\}: {a} }\longrightarrow {\textrm{Cx: }y = {a} +\eta :\mathcal{N}_{\eta}(0,\sigma_{\eta}) }\longrightarrow {\textrm{Rx: } \textrm{compare: }\operatorname{M}({a},\hat{a})\Rightarrow\{p(y|a_n)\}}
		$
"""

import numpy as np
import math

from scipy import signal
from scipy.signal import lti
from scipy.integrate import quad
import scipy.stats
from scipy.fft import fft , fftfreq ,fftshift
import seaborn as sns

import matplotlib.pyplot as plt  # data plotting
from matplotlib import rcParams

from sklearn.neighbors import KernelDensity
from sklearn.model_selection import GridSearchCV

rcParams['figure.figsize']     = [15,6]
rcParams['lines.markersize']   = 9         # marker size in points
plt.rcParams['axes.labelsize'] = 15
plt.rcParams['axes.titlesize'] = 15
plt.rcParams['figure.dpi']     = 180


from IPython.display import display, Math
from matplotlib import colors
from ipywidgets import interact

# funcion para calculo del Histograma
def coloredhist(X, plot=None):

    """
    Crea un histograma coloreado en función de la frecuencia de los valores.

    Parámetros:
    - X: Datos a graficar.
    - plot: Índice del gráfico.
    """
    Nbins = int(1 + 3.322*np.log2(len(X)))  # Cálculo del número de bins usando la regla de Sturges
    # Creación del histograma
    if plot != None:
        n, bins, patches = ax[plot].hist(X, bins=Nbins, density=True)
    else:
        n, bins, patches = ax.hist(X, bins=Nbins, density=True)
    # Normalización de las frecuencias para determinar colores
    fracs = n / n.max()
    norm = colors.Normalize(fracs.min(), fracs.max())
    # Asignación de colores según la frecuencia relativa
    for thisfrac, thispatch in zip(fracs, patches):
        color = plt.cm.viridis(norm(thisfrac))
        thispatch.set_facecolor(color)
    plt.grid()  # Añadir rejilla al gráfico
    return

# Funciones para calcular la pdf de una distribución normal con media positiva y negativa
def normal_distribution_function_r(x,mean,std):
    value = scipy.stats.norm.pdf(x, mean, std)  # Calcular la pdf con media positiva
    return value

def normal_distribution_function_l(x,mean,std):
    value = scipy.stats.norm.pdf(x, -mean, std)  # Calcular la pdf con media negativa
    return value

def norMaxMin(data):
    """
    Normaliza los datos basándose en el valor mínimo y máximo.

    Parámetros:
    - data: Datos a normalizar.

    Retorna:
    - Datos normalizados.
    """
    min_value = np.min(data)  # Encuentra el valor mínimo en los datos
    max_value = np.max(data)  # Encuentra el valor máximo en los datos
    # Normaliza los datos usando los valores mínimo y máximo
    return (data - min_value) / (max_value - min_value)

import os  # Importa el módulo os para interactuar con el sistema operativo

def clearConsole():
    """Limpia la consola o terminal."""
    command = 'clear'  # Comando por defecto para sistemas Unix/Linux
    # Si el sistema operativo es Windows, usa el comando 'cls'
    if os.name in ('nt', 'dos'):
        command = 'cls'
    os.system(command)  # Ejecuta el comando para limpiar la consola

# Funcion para generar los Datos
def generate_data(Nsample,seed=17):
    # Fix the seed to reproduce the results
    rand = np.random.RandomState(seed)
    x = []
    dat = rand.lognormal(0, np.pi/10, Nsample)
    x = np.concatenate((x, dat))
    dat = rand.normal(np.pi, 1, Nsample)
    x = np.concatenate((x, dat))
    return x

"""#### Tx
message = $\{a_n{\in }\mathbb{R}:n {\in} N\}$ --  {alphabet}

$\{p_i\in\mathbb{R}[0,1]\}$
"""

clearConsole() # limpiar la consola

# Generación de datos simbólicos
a1 = 1       # Valor del símbolo a1
a0 = -1      # Valor del símbolo a0
Nsamples = 128 # Muestras de tiempo por símbolo
# Probabilidades de los símbolos
p_0 = 0.99
p_1 = 1 - p_0

Nsymbols = 32  # Número total de símbolos
N = Nsymbols * Nsamples  # Número total de muestras

# Genera una secuencia de símbolos basados en las probabilidades dadas
data = np.random.choice([a0, a1], size=Nsymbols, p=[p_0, p_1])
# Repite cada símbolo según el número de muestras por símbolo
data = np.concatenate([[v]*Nsamples for v in data])
message = data  # Almacena la secuencia completa en la variable 'message'

plt.plot(message)
plt.show()

"""#### Cx
Channel simulation: Additive White Gaussian Noise: $\eta(t),$ with $\mathbb{E}\{\eta(t)\}=0$
"""

# generar señal de ruido y añadirla a la señal original.

#luego de haber aplicado el método de Cross Validation y verificar su funcionamiento, se identificó que este
# presenta un costo computacional muy alto, por ello se de propone la implementación
# de la regla de Silverman la cual genera resultados mucho mas rápidos pero un poco menos precisos

def kde_function(signal_x,eval_points=None,method="Silverman",res=500):
    """
    Aplica el método de KDE gaussiano a la señal proporcionada y ajusta
    automáticamente el ancho de banda mediante una búsqueda en la cuadrícula.

    Parámetros:
    - signal: Señal a la que se aplica KDE.
    - eval_points: puntos de evaluación
    - method: método

    Retorna:
    - eval_points: Vector de evaluación.
    - y_sp: Función de densidad estimada en los puntos de evaluación.
    """
    if eval_points is None:
        eval_points = np.linspace(np.min(signal_x), np.max(signal_x),res)

    if method == 'Silverman':
        n = len(signal_x)
        sigma = np.std(signal_x)
        optimal_bandwidth = (4 / (3 * n)) ** (1 / 5) * sigma

        print(f'Method:{method}\nOptimal bandwidth: {optimal_bandwidth:.2f}')
        kde = scipy.stats.gaussian_kde(signal_x, bw_method=optimal_bandwidth)
        y_sp = kde.pdf(eval_points)

    elif method == 'Cross Validation':
        # Define los posibles valores para el ancho de banda
        bandwidths = np.arange(0.05, 2, 0.05)

        # Crea y configura el objeto para la búsqueda en la cuadrícula
        kde = KernelDensity(kernel='gaussian')

        grid = GridSearchCV(kde, {'bandwidth': bandwidths})
        grid.fit(signal_x.reshape(-1, 1))
        # Obtiene el mejor estimador y su ancho de banda óptimo
        kde_optimal = grid.best_estimator_
        optimal_bandwidth = kde_optimal.bandwidth
        print(f'Optimal bandwidth: {optimal_bandwidth:.2f}')

        log_dens = kde_optimal.score_samples(eval_points.reshape(-1, 1))
        y_sp = np.exp(log_dens)

    else: return

    return (eval_points, y_sp)

# Definición del vector de tiempo
time = np.linspace(0, N, N)
# Cálculo del número de bins usando la regla de Sturges
Nbins = int(1 + 3.322*np.log2(len(data)))
weight = np.ones_like(message) / N  # Pesos para el histograma

# Creación de gráficos
fig, ax = plt.subplots(2, 1, tight_layout=True)

# Generación de ruido gaussiano de media cero
σ2 = 1  # Varianza del ruido
noise = np.random.normal(0, σ2, N)

#ruido como una distribucion raice np.rice



# Estimación de la pdf del ruido
x_train  = generate_data(Nsamples)[:, np.newaxis]

pdf_x, pdf_y = kde_function(noise,method='Cross Validation')

# Visualización del ruido y su pdf
ax[0].plot(time, noise)
ax[1].plot(pdf_x, pdf_y, "-r")
coloredhist(noise, plot=1)
ax[0].set_xlim(0, len(message))

#Chi-Square Normality Test
from scipy import stats

def Test_Chi_Square(data):
    print('Chi-Square Test')
    stat_3, p_3 = stats.chisquare(data)
    x_3 = False
    print(f"stat= {round(stat_3,3)}, p= {round(p_3,5)}")
    if p_3 > 0.05:
        x_3 = True
        print("Probably Gaussian")
    else:
        print("Probably not Gaussian")
    return stat_3, p_3, x_3

Test_Chi_Square(message)

"""Additive Gaussian Channel
$y(t) = \sum_{\forall k}{{a}_n}\operatorname{rect}_{\delta t}(t-k\delta t) +\eta(t)$
"""

# Generación de la señal corrupta sumando el mensaje y el ruido
y = message + noise
y_pdf_x, y_pdf_y = kde_function(y)
fig, ax = plt.subplots(2, 1, tight_layout=True)
ax[0].step(time, y, '-')
ax[0].set_title('Señal corrupta por ruido')
ax[0].set_xlim(0, N)
ax[0].step(time, message, '-r')
coloredhist(y, plot=1)
ax[1].plot(y_pdf_x, y_pdf_y)
plt.show()

Test_Chi_Square(y)

# RX

# Definición de parámetros para el filtro suavizante
R = 3000
C = .0003
num = [1]
den = [R * C, 1]

# Obtención de la respuesta al impulso y función de transferencia del filtro
lti_system = lti(num, den)
t, h = lti_system.impulse(N)
coeff = fftshift(fft(h))
f = fftshift(fftfreq(coeff.shape[0], t[1] - t[0]))

# Aplicación del filtro a la señal corrupta
_, output0_, _ = lti_system.output(y, time)
output_pdf = kde_function(output0_, method = "Cross Validation")

# Creación de gráficos
fig, ax = plt.subplots(3, 1, tight_layout=True)

# Configuración de la disposición de los gráficos
grid = plt.GridSpec(2, 2, wspace=0.15, hspace=0.03)
ax[0] = plt.subplot(grid[0, 0])
ax[0].set_title('Filtro pasabajas RC')
# Visualización de la respuesta en frecuencia del filtro
ax[0].loglog(f, norMaxMin(coeff), 'k')
ax[0].set_xlabel('Freq [$Hz$]')
ax[0].grid()

ax[1] = plt.subplot(grid[0, 1])
# Visualización del oscilograma de las señales de entrada y salida del filtro
ax[1].plot(time, y, label='Entrada')
ax[1].plot(time, output0_, label='Salida Filtrada')
ax[1].set_title('Oscilograma')
#ax[1].set_xlim(0, Nsamples/2)
ax[1].grid(True)
ax[1].set_xlabel('Tiempo [$s$]')
ax[1].set_ylabel('Amplitud')

# Visualización de la pdf estimada de la señal filtrada
ax[2].set_title('pdf')
ax[2].plot(output_pdf[0], output_pdf[1],color='r')
coloredhist(output0_, plot=2)
plt.legend()
plt.show()

"""$\varLambda (y)=  {{p(y|a_1 )}}/{{p(y|a_0 )}} \textrm{   Likelihood ratio}$"""

def error_umbral(umbral,S0,S1):
    # Filtrar los datos dentro de los límites de integración
    Error = []
    std = np.sqrt(σ2)
    S0_pdf = S1
    S1_pdf = S0

    for u in umbral:

        ainf_0 = a1-6*std
        true_values_0 = (S0_pdf[0] >= ainf_0) & (S0_pdf[0] <= u)

        y0_pdf = S0_pdf[1][true_values_0]
        x0_pdf = S0_pdf[0][true_values_0]

        h_0 = np.diff(x0_pdf)  # Calcula el ancho de cada subintervalo
        int_1 = 0.5 * h_0 * (y0_pdf[:-1] + y0_pdf[1:])  # Calcula el área de cada trapecio
        int_1 = np.sum(int_1)

        ainf_1 = a0+6*std
        true_values_1 = (S1_pdf[0] >= u) & (S1_pdf[0] <= ainf_1)

        y1_pdf = S1_pdf[1][true_values_1]
        x1_pdf = S1_pdf[0][true_values_1]

        h_1 = np.diff(x1_pdf)  # Calcula el ancho de cada subintervalo
        int_2 = 0.5 * h_1 * (y1_pdf[:-1] + y1_pdf[1:])  # Calcula el área de cada trapecio
        int_2 = np.sum(int_2)

        Error.append(p_0 * int_1 + p_1 * int_2)

    return Error[::-1]

Simbol_0 = a0 + noise #np.random.normal(a0,np.sqrt(σ2),N)
Simbol_1 = a1 + noise #np.random.normal(a1,np.sqrt(σ2),N)

x_min= min(np.min(Simbol_0),np.min(Simbol_1))
x_max= max(np.max(Simbol_0),np.max(Simbol_1))

x=np.linspace(x_min,x_max,1000)

S0_pdf = kde_function(Simbol_0, eval_points = x, method="Cross Validation")
S1_pdf = kde_function(Simbol_1, eval_points = x, method="Cross Validation")
umbral = kde_function(output0_, eval_points = x, method="Cross Validation")

error = error_umbral(umbral[0],S0_pdf,S1_pdf)

ideal_umbral = umbral[0][np.argmin(error)]
min_error = error[np.argmin(error)]
print(f'umbral ideal: {round(ideal_umbral,4)}\nError: {min_error}')

"""cuando las varianzas son diferentes se obtiene como resultado para el Threshold una ecuacion cuadratica por lo que se obtienen 2 valores distintos. asumiendo entonces que la señal no es simetrica y que la dispersion para cada uno de los simbolos es diferente entonces se procede a encontrar  el valor optimo que me define u la zona de desicion.

![Alt text](image.png)
"""

# calculo simbolico del threshold

import sympy as sp

# Define las variables simbólicas
y = sp.symbols('y', real=True)
mu0, sigma0 = sp.symbols('mu0 sigma0', real=True, positive=True)
mu1, sigma1 = sp.symbols('mu1 sigma1', real=True, positive=True)

# Define las dos funciones de densidad de probabilidad gaussianas para H0 y H1
pdf_H0 = 1 / (sp.sqrt(2 * sp.pi * sigma0**2)) * sp.exp(-(y - mu0)**2 / (2 * sigma0**2))
pdf_H1 = 1 / (sp.sqrt(2 * sp.pi * sigma1**2)) * sp.exp(-(y - mu1)**2 / (2 * sigma1**2))

# Iguala las dos pdfs y resuelve para y
threshold_eq = sp.Eq(pdf_H0, pdf_H1)
solutions = sp.solve(threshold_eq, y)

# Mostrar las soluciones
#print(f"Solutions for y: {solutions}")
display(Math(f"Solutions for y: {solutions}"))

# Se specifica los valores para mu0, mu1, sigma0 y sigma1
mu0_val = -1   # Media para H0
mu1_val = 1   # Media para H1
sigma0_val = 0.2  # Varianza para H0
sigma1_val = 0.5    # Varianza para H1

# Sustituye en las soluciones
numeric_solutions = [sol.evalf(subs={mu0: mu0_val, mu1: mu1_val, sigma0: sigma0_val, sigma1: sigma1_val}) for sol in solutions]
print(f"umbral for y: {numeric_solutions}")

from scipy.stats import gaussian_kde

def binary_threshold(S0_pdf, S1_pdf):
    # Asumimos que S0_pdf[0] y S1_pdf[0] son los valores de x evaluados y S0_pdf[1] y S1_pdf[1] son los valores de la pdf
    # Ordenamos las pdfs para asegurarnos de que estén en el orden correcto
    sorted_indices_S0 = np.argsort(S0_pdf[0])
    sorted_indices_S1 = np.argsort(S1_pdf[0])

    S0_pdf = (S0_pdf[0][sorted_indices_S0], S0_pdf[1][sorted_indices_S0])
    S1_pdf = (S1_pdf[0][sorted_indices_S1], S1_pdf[1][sorted_indices_S1])


    ######
    # calculamos la medias de cada una de las distribuciones
    mean_S0 = np.mean(S0_pdf[0] * S0_pdf[1])
    mean_S1 = np.mean(S1_pdf[0] * S1_pdf[1])

    # calcular la desviacion para cada distribucion

    threshold = (mean_S1 + mean_S0)/2

    return threshold

Simbol_0 = a0 + noise #np.random.normal(a0,np.sqrt(σ2),N)
Simbol_1 = a1 + noise #np.random.normal(a1,np.sqrt(σ2),N)

x_min= min(np.min(Simbol_0),np.min(Simbol_1))
x_max= max(np.max(Simbol_0),np.max(Simbol_1))

x=np.linspace(x_min,x_max,1000)

S0_pdf = kde_function(Simbol_0, eval_points = x, method="Cross Validation")
S1_pdf = kde_function(Simbol_1, eval_points = x, method="Cross Validation")

"""se comprueba que se cumple la condicion


![Alt text](image-1.png)
"""

ideal_threshold = binary_threshold(S0_pdf, S1_pdf)
print(f'Umbral ideal de Neyman-Pearson: {ideal_threshold}')